<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Generic - Hyperspace by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">The Dark Side of the Internet</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="contentmoderation.html">Ep 2</a></li>
						<li><a href="targetedinformation.html">Ep 3</a></li>
						<li><a href="mentalhealth.html">Ep 4</a></li>
						<li><a href="informationarchive.html">Info Archive</a></li>
						<!-- <li><a href="generic.html" class="active">Generic</a></li>
						<li><a href="elements.html">Elements</a></li> -->
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Deepfakes</h1>
							<span class="image fit"><iframe width="560" height="315" src="https://www.youtube.com/embed/b_s6yCNgZyM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></span>
							
							<p>Deep Fakes are a tool to make videos that resemble legitimate events but are actually edited - similar to photoshop. The primary use-case so far has been in pornography, but they can also be used to make videos of anyone - including influential figures like politicians and celebrities - where they can be portrayed saying damaging things. The visual and audio components of these videos can be faked using a form of artificial intelligence called deep learning.</p>
							<h2><b>Episode Transcript</b></h2>
							
							<p>
Aatish: [00:00:00] Welcome back to The Dark Side of the Internet. I'm your host Aatish <br><br>
Vikram: and I'm Vikram, and we hope you're doing well today. Today's episode will focus primarily on deep fakes and the implication they have on our lives. Deep fakes are a growing concern in the digital age. These are videos, images or audio files that have been manipulated using artificial intelligence to create fake content that appears real.<br><br>
While deep fakes can be entertaining or amusing, they can also be harmful and have dangerous implications. Today we'll discuss the dangers of deep fakes and the uses in social media.<br><br>

So what exactly are deep fakes?<br><br>
Aatish: deep fakes are artificial intelligence algorithms, which are designed to perform a sort of advanced face swap. There's a lot of ways to make this happen, but the basic structure in general is the algorithm [00:01:00] learns the movements and patterns in a person's face, and then maps it to someone else's face.<br><br>
This can happen in images and videos.<br><br>
Vikram: Huh, wait, how does it do it in videos per se. <br><br>
Aatish: in videos? It just takes a bunch of frames, like substantial image, except multiple of them since it's a video. <br><br>
Vikram: Oh, and he stitches it together to make the video.<br><br>
Aatish: That's right. <br><br>
Vikram: That's actually really interesting. <br><br>
Aatish: Also, they can be used to change people's voices.<br><br>
Vikram: Mm-hmm<br><br>
Aatish: so, you know, you got the video, you got the visual component, and you also have the audio component. And together, you know, that's, that's pretty scary. 
<br><br>
Vikram: Right, that can make a quite convincing video if used in the wrong place, I think. 
<br><br>
Aatish: Absolutely. 
<br><br>
Vikram: So what about like the, the general public? Are people aware of deep fakes that much or 
<br><br>
Aatish: It's becoming more prevalent in society and so, you know, people are taking notice, but in general, not really.
<br><br>
However, um, recently Google Trends actually showed a major spike in recent searches about deep fakes um, that's been since about 2015. So, you know, we've got good coverage of this [00:02:00] issue, but you know, a lot of the population still needs to know. 
<br><br>
Vikram: So it's still a relatively new technology, I would say. Right?
<br><br>
Aatish: Absolutely. It's still getting better and becoming more dangerous. 
<br><br>
Vikram: Right. I think you mentioned Google Trends, right? I'm just, just looking at it now. I think 2021. It was about 41 on their benchmark. And just looking at 2023, it's about a hundred. So that's almost a double increase in interest over just two years. Do you know why that might be the case?
<br><br>
Aatish: That's correct. Um, you know, it's because there's a lot more deep fakes these days that are really catching people's attention. 
<br><br>
Vikram: Mm-hmm 
<br><br>
Aatish: um, you know, one example is the Tom Cruise Deep fake. You know, that was the news, you know, it was everywhere. There's also Kendrick Lamar's music video, you know, he's got Will Smith, Kobe Bryant, all kinds of people on that. 
<br><br>
Vikram: I remember that one. Yeah. That was really funny. 
<br><br>
Aatish: Exactly. Pretty cool. There's also been a few scandals, so you know, more serious stuff. Um, world leaders like Barack Obama and Donald Trump are just saying things they've never said and you know, some people might even believe it.
Vikram: That's actually pretty scary if you think about it. If you have [00:03:00] our president saying things that they never said, um, like, so where do you draw the line in this really? Cause if you have on one end you have Tom Cruise getting getting deep faked for most likely harmless cases. And on the other side you have Obama and the president of the United States saying things that they probably never said.
So where do you draw the line of like, what is entertainment and what has an actual impact on society and the potential to skew people's opinions? So I think that's like a really big issue, I think, in deep fakes as it is right now. Would you agree? 
<br><br>
Aatish: Absolutely. Absolutely.
<br><br>
So, you know, I've been exploring all the negative consequences on like, you know, society as a whole with deep fakes, but do you know of anything like productive that's been going on? 
<br><br>
Vikram: Yeah, yeah, for sure. There's, there's quite a bit actually. So if you think about it in the film industry, especially cuz that deals heavily with videos, right?
<br><br>
In films, in many films, it's been [00:04:00] used to actually create realistic CGI characters already. So I'm, I'm not sure if you've seen the Mandalorian, but in, uh, the second season, towards the end they have Luke Skywalker, um, as a, as a deep fake, um, in instead of having. , mark Hamel, the actor, play the part. They, they saved a lot of money by having, instead of having the deep fake version of him.
So that's it comparatively is cost saving because you don't have to reshoot or have an expensive actor on the set for that day. 
<br><br>
Aatish: Got it. Um, but you know, outside of the entertainment industry, is there any use cases you said? 
<br><br>
Vikram: Yeah. Um, so in the medical industry, actually, they've been used for training purposes cuz they can offer simulation, showing doctors how to perform complex surgeries.
So a lot of companies, specifically one in California called Retrace, have used it to fill in blank data used in x-rays. So the, um, they've also used it to specifically predict the amount of bone marrow in certain parts of the mouth, which allows the patient to understand, um, their bone structure [00:05:00] without having to pay for a full x-ray.
In addition, they've also used it to extend x-ray. to show parts of the body that might not have been covered in the x-rays done, but the patient might want. So once again, it just, it saves costs for the patient or for a film industry. 
<br><br>
Aatish: That's wild. Do you think this could also be deployed in like an education context? Like, you know, I get to see Abe Lincoln back from the dead. 
<br><br>
Vikram: Yeah. Yeah, for sure. Especially with the way that education has been going. Recently with the use of multimedia teachers have been using deep fakes to bring dead historical figures back to life. And if you think about it really helps students to learn in an interactive manner.
<br><br>
Cause if you have, um, instead of like an actor playing George Washington, if you have George Washington, like a painting of him actually talking to you and telling him about his life, that's, that's, that can be really helpful and conducive for a learning environment for many of the kids. 
<br><br>
Aatish: That's so cool. I definitely would've enjoyed history class if I had that. Um, but you know, do you think this could be used in like a commercial [00:06:00] context. 
<br><br>
Vikram: Definitely. Yeah. So another use of deep fakes is for advertising and marketing, which companies can use. Um, Dix can be used to create realistic looking commercials or even advertisement that can be more engaging and memorable than traditional ads.
So, yeah, those are all the, the sort of good things about deep fakes. But I mean, we are called The Dark Side of the Internet, so can you tell us about the cons? 
<br><br>
Aatish: For sure, for sure. There is plenty. So, um, you know, first and foremost the spread of fake news and propaganda. Um, by both, you know, just random people and politicians.
And what's worse is now if there's a real video, you know, anyone can just say, oh, that was deep faked. You know, for example, Barack Obama said about people in like, uh, hard hit areas that they just cling to guns and religion that Mitt Romney said 47% of citizens in the [00:07:00] US are just happy to depend on the government.
And finally, Hillary Clinton called Trump supporters deplorables. So, you know, these are all things that are very detrimental to their image, but now they can just claim, oh yeah, that wasn't real. 
<br><br>
Vikram: So did, did they claim that they were like, so Obama, Romney, and Clinton, did they all say those things? 
<br><br>
Aatish: They did not.
However there's potential that they can, and in the future they probably will. 
<br><br>
Vikram: Mm-hmm. 
<br><br>
Aatish: finally, you know, um, if one of these videos is fake and distributed, it's a lie and lies travel a lot faster than the truth already. Detection models lag behind the, uh, deep faking models. So, you know, deep fakes get more and more advanced and we can't catch them in time.
Um, also the ratio of people working on creating deep fake models to detecting them is like a hundred to one. So, you know, you got a hundred engineers for every one person trying to figure out what these a hundred people are doing. That's, you know, clearly not gonna work. And um, you know, we see some evidence that deep fakes already working like a serious context.
For example, there was [00:08:00] a coup in Gabon that was, that deep fakes contributed to. And now in Malaysia, people are also trying to drive out the economic affairs Minister outta office. It didn't work. However, you know, it's a dangerous precedent. 
<br><br>Vikram: Right, right. And if these videos do get out there into the public, what damage could it do?
<br><br>Aatish: Well, you know, people actually get a lot of their news from social media. Almost 50% of people age 18 to 29. Um, 40% of people from 30 to 49. So, you know, that's a big, uh, amount of the population that just relies on social media for the truth. Um, and you know, the bad part about this is people who do this tend to be less educated and wealthy, which means they're less likely to even know that deep fakes exist.
And, uh, this is straight from the Pew Research Centers. You know, it's, it's very true and very reputable. 
<br><br>Vikram: So are there any more nefarious purposes of deep fakes videos other than just manipulating the public? 
<br><br>Aatish: Absolutely. Unfortunately, 90% [00:09:00] 96% of DEG videos are pornography, um, and revenge porn is actually very commonplace.
In fact, representative Katy Hill lost her career due to this issue. So it's, it's really serious. 
<br><br>Vikram: Yeah. That's just disgusting. And I honestly, if you think about it, I doubt celebrities gave their consent to have their face be put in, uh, nude videos like that. So there's gotta be some ethical concerns surrounding deep fakes right?
<br><br>Aatish: Yeah, absolutely. Um, but unfortunately, you know, our laws and legislations just lagging behind, so it's just a free for all, wild west. 
<br><br>Vikram: I feel like that, that happens a lot when you have new technologies that are introduced. Right? 
<br><br>Aatish: Right. Absolutely. Um, although, you know, there are some ways we can figure out deep fakes are deep fakes.
Um, there are current AI models, you know, detection models, but there's that a hundred to one issue. But then also, like, you know, as a normal viewer, um, they're not perfect. They're not perfect. You can, you know, look for weird lip movements. Maybe it's, there's like not enough blinking or, you know, the glasses don't have enough glare or too much glare. So, you know, light is not being tracked properly. [00:10:00] Um, but the problem is, you know, people need to know that deep fakes exist so that they concentrate enough to, you know, detect them. Cuz you know, if I think a video is real, I'm not gonna second guess it. 
<br><br>Vikram: Right. And another issue is the fact that there's, there, there's so much deep fake videos out there and it's gotten to the point where the general public can make one with fair ease, right?
<br><br>Aatish: Absolutely. Um, you know, the average person now can have a massive impact. Um, not just large corporations, but you know, the government and then also just like people like you.
<br><br>Vikram: Right 
<br><br>Aatish: and you know, this is just like the tip of the iceberg. There's always new stories coming out, like the unfortunate case of Katy Hill from reported by the atlantic.
<br><br>Vikram: Right.
What about a specific use case within social media itself? 
<br><br>Aatish: Mm-hmm. 
Absolutely. So, you know, there's the misinformation propaganda issue. Um, I could just [00:11:00] make a video of a politician saying really anything, um, or just a false image of an event that never even happened. Uh, and because, you know, social media primarily relies on the visual medium, um, it's very vulnerable.
<br><br>Vikram: That makes sense. Right. But what can social media companies do to limit the spread of deep fake based information. 
<br><br>Aatish: Um, so you know, there have been some collaborations. For example, Facebook collaborate with Reuters to identify and remove deep fakes. Then Twitter just relies on the public. They have a notification system where people can flag videos.
And then even Google, although it's not a social media company, does have a big stake in this. So they've also given out their data sets of deep fake videos and not deep fake videos. So, you know, people can make their detection models using that. 
<br><br>Vikram: Mm-hmm. , and I think you mentioned politics briefly there, right?
I think it, it's especially dangerous in politics. Cause if you can manipulate the public opinion with, like you said, um, fake videos of politicians saying something, it can sway the [00:12:00] vote in, in certain elections, which could be classified as an election interference. Um, and it's gone to the point where actually Gavin Uson, uh, signed a law making it illegal to make deep fakes of politicians 60 days before an election.
<br><br>Aatish: What do you think about the future of this technology? Could it be good, bad? What do you think?
<br><br>Vikram: So I think it's, it's mixed, right? So you definitely have the potential for deep fakes to become more sophisticated so they can become more realistic off of less training photos. In addition, you could have more realistic audio. 
<br><br>Aatish: Yeah. Um, I, you know, I actually read a story lately where someone managed to rob a bank by impersonating the ceo.
So, you know, it's things like that, like you said, that are really, really dangerous.
<br><br>Vikram: That's wild. And it just goes to show that there's a constant need for continued research and development of technology and to mitigate the negative consequences of deep fakes. And the possible legal [00:13:00] implications of deep fakes on our society.
<br><br>So how can it affect the privacy and security of individuals like you and I, and also legal issues such as defamation because deep fakes can be used to damage someone's reputation, career, or even personal life. I mean, for instance, someone could create a deep fake video of their ex-partners saying or doing something harmful, which could lead to serious consequences such as job loss or public humiliation. This can also be used as a form of cyber bullying, harassment, or even revenge porn in some serious cases.
<br><br>The guardian showed us how amazing this technology really is, but clearly we have a lot to do to mitigate the negative impacts of it because deep fakes have a dangerous implication for individuals society and national. Like, while deep fakes can be entertaining, they can also be harmful and lead to serious consequences.
<br><br>It's, it's certainly crucial to be aware of the potential dangers of deep fakes and [00:14:00] take steps to prevent their spread. This includes educating yourself and others on deep fakes, supporting research into deep fake detection and holding social media platforms accountable for their roles in spreading deep fakes.
<br><br>Together we can work to prevent their harmful effects of deep fakes, and protect ourselves from their dangers. And for you to make this happen, you can share our podcast. 
<br><br>Thank you for listening, and this has been Vikram 
<br><br>Aatish: This is Aatish and we are The Dark Side of the Internet.[00:15:00] 

							</p>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>